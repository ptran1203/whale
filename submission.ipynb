{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of 5 models\n",
      "Ensemble from 5 models ['infer\\\\b5_768_full_m0.4_lb0.815', 'infer\\\\b5_data15_lb0.822', 'infer\\\\b5_data20_m0.6_lb0.826', 'infer\\\\b5_pseudo_lb0.827', 'infer\\\\v2m_768_lb819']\n",
      "Ensemble from 5 models ['infer\\\\b5_768_full_m0.4_lb0.815', 'infer\\\\b5_data15_lb0.822', 'infer\\\\b5_data20_m0.6_lb0.826', 'infer\\\\b5_pseudo_lb0.827', 'infer\\\\v2m_768_lb819']\n"
     ]
    }
   ],
   "source": [
    "from utils import pickle_load\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from evaluate import compute_sim, evaluate, l2norm_numpy\n",
    "from evaluate import dict2list\n",
    "\n",
    "def l2norm(embs):\n",
    "    return {k: v/np.linalg.norm(v) for k, v in embs.items()}\n",
    "\n",
    "infer_dirs = glob('infer/*')[:]\n",
    "# infer_dirs = infer_dirs[1:]\n",
    "infer_dirs = [x for x in infer_dirs if 'bk' not in x]\n",
    "infer_dirs = [x for x in infer_dirs if 'pseudo' in x]\n",
    "weights = [1.0] * len(infer_dirs)\n",
    "# weights = [1.0, 0.5, 0.5]\n",
    "\n",
    "print(f\"Ensemble of {len(infer_dirs)} models\")\n",
    "\n",
    "submit_file = f'submission.csv'\n",
    "if os.path.exists('D:/whale_data/train_images-384-384/train_images-384-384'):\n",
    "    train_img_dir = 'D:/whale_data/train_images-384-384/train_images-384-384'\n",
    "else:\n",
    "    train_img_dir = '/Users/macbook/works/train_images-384-384'\n",
    "\n",
    "norm=True\n",
    "method = 'cat'\n",
    "\n",
    "def get_emb(infer_dirs, subset, weights):\n",
    "    li = []\n",
    "    for infer_dir in infer_dirs:\n",
    "        if isinstance(subset, str):\n",
    "            embs = pickle_load(f\"{infer_dir}/{subset}_emb.pkl\")\n",
    "        else:\n",
    "            embs = {}\n",
    "            for s in subset:\n",
    "                p = f\"{infer_dir}/{s}_emb.pkl\"\n",
    "                if os.path.exists(p):\n",
    "                    embs = {**embs, **pickle_load(p)}\n",
    "                elif s != \"val\":\n",
    "                    raise FileNotFoundError(p)\n",
    "\n",
    "        li.append(embs)\n",
    "\n",
    "    if len(li) == 1:\n",
    "        return li[0]\n",
    "\n",
    "    print(f'Ensemble from {len(infer_dirs)} models {infer_dirs}')\n",
    "    li0 = li[0]\n",
    "    di = {}\n",
    "    for k in li0.keys():\n",
    "        di[k] = []\n",
    "        for i in range(len(li)):\n",
    "            e = li[i][k]\n",
    "            if norm:\n",
    "                e = e / np.linalg.norm(e)\n",
    "            di[k].append(e)\n",
    "        \n",
    "        if method == 'cat':\n",
    "            di[k] = np.concatenate(di[k], 0)\n",
    "        else:\n",
    "            # Mean\n",
    "            di[k] = np.mean(di[k], 0)\n",
    "\n",
    "    return di\n",
    "\n",
    "train_df = pd.read_csv('data/train_kfold.csv')\n",
    "\n",
    "run_val = False\n",
    "try:\n",
    "    train_embs = get_emb(infer_dirs, 'train', weights)\n",
    "    test_embs = get_emb(infer_dirs, 'test', weights)\n",
    "    val_embs = get_emb(infer_dirs, 'val', weights)\n",
    "\n",
    "    train_embs = l2norm(train_embs)\n",
    "    test_embs = l2norm(test_embs)\n",
    "    val_embs = l2norm(val_embs)\n",
    "    run_val = True\n",
    "    print(len(train_embs) + len(val_embs))\n",
    "except:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = train_df[train_df.subset == 'test'].reset_index()\n",
    "val_map = dict(zip(val_df.image, val_df.individual_id))\n",
    "train_map = dict(zip(train_df.image, train_df.individual_id))\n",
    "spec_map = dict(zip(train_df.image, train_df.species))\n",
    "id_map = {**train_map, **val_map}\n",
    "# val_imgs = val_df.image.unique()\n",
    "# val_embs = {k: train_embs[k] for k in val_imgs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import compute_sim, evaluate, map_per_image, compute_simv2\n",
    "from evaluate import *\n",
    "\n",
    "if run_val:\n",
    "    score, val_sim_df = evaluate(train_df, train_embs, val_embs, norm=True)\n",
    "    val_sim_df[\"gt\"] = val_sim_df.image.map(val_map)\n",
    "    val_sim_df[\"map\"] = val_sim_df.apply(lambda row: map_per_image(row[\"gt\"], row.predictions.split(\" \")), axis=1)\n",
    "    val_sim_df = val_sim_df.sort_values(\"map\")\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_val:\n",
    "    train_k, train_v = dict2list(train_embs)\n",
    "    test_k, test_v = dict2list(val_embs)\n",
    "    class_count_df = train_df.groupby('individual_id').size().to_frame('count').reset_index()\n",
    "    class_count = dict(zip(class_count_df.individual_id, class_count_df['count']))\n",
    "    print(train_v.shape, test_v.shape)\n",
    "    train_v = l2norm_numpy(train_v)\n",
    "    test_v = l2norm_numpy(test_v)\n",
    "    train_ids = np.unique([train_map[x] for x in train_k])\n",
    "    allowed = []\n",
    "    for i, k in enumerate(test_k):\n",
    "        if train_map[k] in train_ids:\n",
    "            allowed.append(i)\n",
    "    test_k, test_v = [test_k[i] for i in allowed], [test_v[i] for i in allowed]\n",
    "    cosines = np.matmul(test_v, train_v.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_border(img, color):\n",
    "    bordersize = 14\n",
    "    return cv2.copyMakeBorder(\n",
    "        img,\n",
    "        top=bordersize,\n",
    "        bottom=bordersize,\n",
    "        left=bordersize,\n",
    "        right=bordersize,\n",
    "        borderType=cv2.BORDER_CONSTANT,\n",
    "        value=color\n",
    "    )\n",
    "\n",
    "if run_val:\n",
    "    c = 0\n",
    "    for i, scores in enumerate(cosines):\n",
    "        if np.random.rand() < 0.1:\n",
    "            top = 7\n",
    "            sort_idx = np.argsort(-scores)\n",
    "            topk = [train_k[j] for j in sort_idx[:top]]\n",
    "            topk_score = [scores[j] for j in sort_idx[:top]]\n",
    "            topk_id = [train_map[x] for x in topk]\n",
    "            qid = test_k[i]\n",
    "            gt = train_map[qid]\n",
    "            c+=1\n",
    "            imgs = [cv2.imread(f'{train_img_dir}/{qid}')[:,:,::-1]]\n",
    "            for k, l in zip(topk, topk_id):\n",
    "                im = cv2.imread(f'{train_img_dir}/{k}')[:,:,::-1]\n",
    "                im = add_border(im, color=(255, 0, 0) if gt != l else (0, 128, 0))\n",
    "                imgs.append(im)\n",
    "            # Show image\n",
    "            fig = plt.figure(figsize=(25, 4))\n",
    "            columns = top + 1\n",
    "            rows = 1\n",
    "            for i2 in range(0, columns*rows):\n",
    "                fig.add_subplot(rows, columns, i2+1)\n",
    "                plt.title(f'{gt} {id_map[qid]}' if i2 == 0 else f'{topk_score[i2 - 1]:.2f} {id_map[topk[i2 -1]]}')\n",
    "                plt.imshow(imgs[i2])\n",
    "                plt.axis('off')\n",
    "            \n",
    "            plt.show()\n",
    "            \n",
    "        if c == 4:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_sim_df\n",
    "\n",
    "# sim_df = compute_sim(train_df, {**train_embs, **val_embs}, test_embs, thr=0.5, norm=True)\n",
    "# sim_df[[\"image\", \"predictions\"]].to_csv('submission.csv', index=False)\n",
    "# sim_df.head()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "n_neighbors = 100\n",
    "knn = NearestNeighbors(n_neighbors=n_neighbors,metric='cosine')\n",
    "\n",
    "# diff fold\n",
    "db_embs = get_emb(infer_dirs, ['train', 'val'], weights)\n",
    "test_embs = get_emb(infer_dirs, 'test', weights)\n",
    "\n",
    "# db_embs = {**train_embs, **val_embs}\n",
    "\n",
    "train_k, train_v = dict2list(db_embs)\n",
    "test_k, test_v = dict2list(test_embs)\n",
    "knn.fit(train_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances, idxs = knn.kneighbors(test_v, n_neighbors, return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_df = []\n",
    "train_k = np.asarray(train_k)\n",
    "img2id = dict(zip(train_df.image, train_df.individual_id))\n",
    "\n",
    "for i in tqdm(range(len(test_k))):\n",
    "    dist, idx = distances[i], idxs[i]\n",
    "    for d, id in zip(dist, idx):\n",
    "        img_id = train_k[id]\n",
    "        tar = img2id[img_id]\n",
    "        test_df.append([test_k[i], tar, d]) \n",
    "    \n",
    "test_df = pd.DataFrame(test_df, columns=['image', 'target', 'distances'])\n",
    "test_df['confidence'] = 1-test_df['distances']\n",
    "test_df = test_df.groupby(['image','target']).confidence.max().reset_index()\n",
    "test_df = test_df.sort_values('confidence',ascending=False).reset_index(drop=True)\n",
    "test_df.to_csv('test_neighbors.csv')\n",
    "test_df.image.value_counts().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "sample_list = ['938b7e931166', '5bf17305f073', '7593d2aee842', '7362d7a01d00','956562ff2888']\n",
    "\n",
    "for i,row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    if row.image in predictions:\n",
    "        if len(predictions[row.image])==5:\n",
    "            continue\n",
    "        predictions[row.image].append(row.target)\n",
    "    elif row.confidence >= 0.5  :\n",
    "        predictions[row.image] = [row.target,'new_individual']\n",
    "    else:\n",
    "        predictions[row.image] = ['new_individual',row.target]\n",
    "\n",
    "for x in tqdm(predictions):\n",
    "    if len(predictions[x])<5:\n",
    "        remaining = [y for y in sample_list if y not in predictions]\n",
    "        predictions[x] = predictions[x]+remaining\n",
    "        predictions[x] = predictions[x][:5]\n",
    "    predictions[x] = ' '.join(predictions[x])\n",
    "    \n",
    "predictions = pd.Series(predictions).reset_index()\n",
    "predictions.columns = ['image','predictions']\n",
    "predictions.to_csv(submit_file,index=False)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0 == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST CV\n",
    "\n",
    "n_neighbors = 100\n",
    "knn = NearestNeighbors(n_neighbors=n_neighbors,metric='cosine')\n",
    "\n",
    "train_k, train_v = dict2list(train_embs)\n",
    "val_k, val_v = dict2list(val_embs)\n",
    "knn.fit(train_v)\n",
    "\n",
    "distances, idxs = knn.kneighbors(val_v, n_neighbors, return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "val_df = []\n",
    "train_k = np.asarray(train_k)\n",
    "img2id = dict(zip(train_df.image, train_df.individual_id))\n",
    "\n",
    "for i in tqdm(range(len(val_k))):\n",
    "    dist, idx = distances[i], idxs[i]\n",
    "    for d, id in zip(dist, idx):\n",
    "        img_id = train_k[id]\n",
    "        tar = img2id[img_id]\n",
    "        val_df.append([val_k[i], tar, d]) \n",
    "    \n",
    "val_df = pd.DataFrame(val_df, columns=['image', 'target', 'distances'])\n",
    "val_df['confidence'] = 1-val_df['distances']\n",
    "val_df = val_df.groupby(['image','target']).confidence.mean().reset_index()\n",
    "val_df = val_df.sort_values('confidence',ascending=False).reset_index(drop=True)\n",
    "val_df.to_csv('val_neighbors.csv')\n",
    "val_df.image.value_counts().value_counts()\n",
    "\n",
    "val_pred = {}\n",
    "sample_list = ['938b7e931166', '5bf17305f073', '7593d2aee842', '7362d7a01d00','956562ff2888']\n",
    "\n",
    "for i,row in tqdm(val_df.iterrows(), total=len(val_df)):\n",
    "    if row.image in val_pred:\n",
    "        if len(val_pred[row.image])==5:\n",
    "            continue\n",
    "        val_pred[row.image].append(row.target)\n",
    "    elif row.confidence >= 0.5:\n",
    "        val_pred[row.image] = [row.target,'new_individual']\n",
    "    else:\n",
    "        val_pred[row.image] = ['new_individual',row.target]\n",
    "\n",
    "for x in tqdm(val_pred):\n",
    "    if len(val_pred[x])<5:\n",
    "        remaining = [y for y in sample_list if y not in val_pred]\n",
    "        val_pred[x] = val_pred[x]+remaining\n",
    "        val_pred[x] = val_pred[x][:5]\n",
    "    val_pred[x] = ' '.join(val_pred[x])\n",
    "    \n",
    "val_pred = pd.Series(val_pred).reset_index()\n",
    "val_pred.columns = ['image','predictions']\n",
    "val_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = np.unique([train_map[x] for x in train_k])\n",
    "allowed = []\n",
    "for i, k in enumerate(val_k):\n",
    "    if train_map[k] in train_ids:\n",
    "        allowed.append(train_map[k])\n",
    "\n",
    "val_targets_df = pd.DataFrame(val_k)\n",
    "val_targets_df.columns = ['image']\n",
    "val_targets_df['target'] = val_targets_df['image'].map(train_map)\n",
    "val_targets_df.loc[~val_targets_df.target.isin(allowed),'target'] = 'new_individual'\n",
    "val_targets_df.head(10)\n",
    "\n",
    "all_preds = dict(zip(val_pred['image'], val_pred['predictions']))\n",
    "th = 0.6\n",
    "for i,row in val_targets_df.iterrows():\n",
    "        target = row.target\n",
    "        preds = all_preds[row.image].split(\" \")\n",
    "        val_targets_df.loc[i,th] = map_per_image(target,preds)\n",
    "cv = val_targets_df[th].mean()\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d4bc70c65d06e1543861fe65a0e2a7420176491a9e60a2c8babf4d7456b2d28"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
